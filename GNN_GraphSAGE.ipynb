{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMHanEzlrKX9pG46gCmppJC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BonanYang/GNN/blob/main/GNN_GraphSAGE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UmN5U_32XvLa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.sparse as sp\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import scipy.sparse as sp\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import scipy.sparse as sp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve('https://data.dgl.ai/dataset/reddit.zip', 'reddit.zip')\n",
        "\n",
        "with zipfile.ZipFile('reddit.zip', 'r') as z:\n",
        "    z.extractall('reddit/')"
      ],
      "metadata": {
        "id": "4QnKDd6lBKTn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data at a glance"
      ],
      "metadata": {
        "id": "Py0VfKrkGrJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.load('reddit/reddit_data.npz')\n",
        "graph = np.load('reddit/reddit_graph.npz')"
      ],
      "metadata": {
        "id": "BCTPFLDqYAnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.DataFrame(data['feature'][:10, :10]).round(2)\n",
        "df1['label'] = data['label'][:10]\n",
        "print(df1)"
      ],
      "metadata": {
        "id": "tkYQO670Y33W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.iloc[0]"
      ],
      "metadata": {
        "id": "9MvtYzYqo14S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data['feature'])\n",
        "df['label'] = data['label']"
      ],
      "metadata": {
        "id": "1yJwrByhonRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[0]"
      ],
      "metadata": {
        "id": "1guBN2xNpjA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adj = sp.load_npz('reddit/reddit_graph.npz')\n",
        "print(type(adj))\n",
        "print(adj.shape)\n",
        "print(adj.nnz)"
      ],
      "metadata": {
        "id": "iLkTiOLhn_20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MLP"
      ],
      "metadata": {
        "id": "CjSdyqYK-d4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "data = np.load('reddit/reddit_data.npz')\n",
        "features = torch.FloatTensor(data['feature'])\n",
        "labels = torch.LongTensor(data['label'])\n",
        "node_types = data['node_types']\n",
        "\n",
        "train_idx = np.where(node_types == 1)[0]\n",
        "val_idx = np.where(node_types == 2)[0]\n",
        "test_idx = np.where(node_types == 3)[0]\n",
        "\n",
        "class RedditDataset(Dataset):\n",
        "    def __init__(self, idx):\n",
        "        self.idx = idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        node_id = self.idx[i]\n",
        "        return features[node_id], labels[node_id]\n",
        "\n",
        "train_loader = DataLoader(RedditDataset(train_idx), batch_size=1024, shuffle=True)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, out_dim)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        return self.fc3(x)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = MLP(602, 256, 41).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(batch_x), batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_out = model(features[val_idx].to(device))\n",
        "        val_acc = (val_out.argmax(1) == labels[val_idx].to(device)).float().mean().item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Loss: {total_loss/len(train_loader):.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_out = model(features[test_idx].to(device))\n",
        "    test_acc = (test_out.argmax(1) == labels[test_idx].to(device)).float().mean().item()\n",
        "print(f\"\\nTest Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "U8BKkoYwuS2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GraphSAGE"
      ],
      "metadata": {
        "id": "xIOIFtJU-lT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "data = np.load('reddit/reddit_data.npz')\n",
        "adj = sp.load_npz('reddit/reddit_graph.npz').tocsr()"
      ],
      "metadata": {
        "id": "LmHiFzbZCvqX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphSAGELayer(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.W = nn.Linear(in_dim, out_dim)\n",
        "        self.B = nn.Linear(in_dim, out_dim)\n",
        "\n",
        "    def forward(self, X, self_idx, neigh_lists):\n",
        "        neighbor_feats = []\n",
        "        for neighs in neigh_lists:\n",
        "            if len(neighs) == 0:\n",
        "                neighbor_feats.append(torch.zeros(X.size(1), device=X.device))\n",
        "            else:\n",
        "                neighbor_feats.append(X[neighs].mean(dim=0))\n",
        "        neighbor_feats = torch.stack(neighbor_feats)\n",
        "        return self.W(neighbor_feats) + self.B(X[self_idx])\n",
        "\n",
        "\n",
        "class GraphSAGE(nn.Module):\n",
        "    def __init__(self, in_dim, hid_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.sage1 = GraphSAGELayer(in_dim, hid_dim)\n",
        "        self.sage2 = GraphSAGELayer(hid_dim, out_dim)\n",
        "        self.S1, self.S2 = 25, 10\n",
        "\n",
        "    def sample(self, nodes, adj_list, size):\n",
        "        sampled = {}\n",
        "        for n in nodes:\n",
        "            neighs = adj_list[n]\n",
        "            if len(neighs) > size:\n",
        "                neighs = np.random.choice(neighs, size, replace=False).tolist()\n",
        "            sampled[n] = neighs\n",
        "        return sampled\n",
        "\n",
        "    def forward(self, X, batch, adj_list):\n",
        "        batch = list(batch)\n",
        "        batch_neighs = self.sample(batch, adj_list, self.S2)\n",
        "        L1_nodes = set(batch)\n",
        "        for neighs in batch_neighs.values():\n",
        "            L1_nodes.update(neighs)\n",
        "        L1_nodes = list(L1_nodes)\n",
        "        L1_neighs = self.sample(L1_nodes, adj_list, self.S1)\n",
        "        H1 = F.relu(self.sage1(X, L1_nodes, [L1_neighs[n] for n in L1_nodes]))\n",
        "\n",
        "        L1_map = {n: i for i, n in enumerate(L1_nodes)}\n",
        "        batch_idx = [L1_map[n] for n in batch]\n",
        "        neigh_idx = [[L1_map[nb] for nb in batch_neighs[n]] for n in batch]\n",
        "        H2 = self.sage2(H1, batch_idx, neigh_idx)\n",
        "\n",
        "        return H2"
      ],
      "metadata": {
        "id": "ZwIrax5y-no_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.FloatTensor(data['feature'])\n",
        "y = torch.LongTensor(data['label'])\n",
        "node_types = data['node_types']\n",
        "\n",
        "train_idx = np.where(node_types == 1)[0]\n",
        "val_idx = np.where(node_types == 2)[0]\n",
        "test_idx = np.where(node_types == 3)[0]\n",
        "\n",
        "adj_list = [adj[i].indices.tolist() for i in range(adj.shape[0])]\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "X = X.to(device)\n",
        "y = y.to(device)\n",
        "\n",
        "model = GraphSAGE(602, 256, 41).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "batch_size = 512\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    np.random.shuffle(train_idx)\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(0, len(train_idx), batch_size):\n",
        "        batch_nodes = train_idx[i:i+batch_size]\n",
        "        out = model(X, batch_nodes, adj_list)\n",
        "        loss = F.cross_entropy(out, y[batch_nodes])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_out = model(X, val_idx, adj_list)\n",
        "        val_acc = (val_out.argmax(1) == y[val_idx]).float().mean().item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Loss: {total_loss*batch_size/len(train_idx):.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_out = model(X, test_idx, adj_list)\n",
        "    test_acc = (test_out.argmax(1) == y[test_idx]).float().mean().item()\n",
        "print(f\"\\nTest Acc: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "gwpqsds_-kkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"max node in adj_list: {max(max(adj_list[i]) if adj_list[i] else 0 for i in range(len(adj_list)))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBs67zLAGFMa",
        "outputId": "15b247c9-514c-442b-b381-6a75b518c6bc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: torch.Size([232965, 602])\n",
            "max node in adj_list: 232964\n"
          ]
        }
      ]
    }
  ]
}